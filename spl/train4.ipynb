{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "initialize_model_parallel() got an unexpected keyword argument 'model_parallel_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m     dist\u001b[38;5;241m.\u001b[39mdestroy_process_group()\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 113\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m dist\u001b[38;5;241m.\u001b[39minit_process_group(backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnccl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Initialize model parallel\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m fs_init\u001b[38;5;241m.\u001b[39minitialize_model_parallel(model_parallel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adjust model_parallel_size based on your setup\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m     46\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikipedia\u001b[39m\u001b[38;5;124m\"\u001b[39m, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20240401\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain[:5\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: initialize_model_parallel() got an unexpected keyword argument 'model_parallel_size'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import the model and tokenizer\n",
    "from model import Transformer, ModelArgs\n",
    "from tokenizer import Tokenizer\n",
    "\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "\n",
    "class WikipediaDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokens = self.tokenizer.encode(text, bos=True, eos=True)\n",
    "        tokens = tokens[:self.seq_len] + [self.tokenizer.pad_id] * (self.seq_len - len(tokens))\n",
    "        return torch.tensor(tokens)\n",
    "\n",
    "def main():\n",
    "    # Set environment variables for distributed training\n",
    "    os.environ['RANK'] = '0'\n",
    "    os.environ['WORLD_SIZE'] = '1'\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # Initialize the distributed environment\n",
    "    dist.init_process_group(backend='nccl')\n",
    "\n",
    "    # Initialize model parallel\n",
    "    fs_init.initialize_model_parallel(model_parallel_size_=1)  # Adjust model_parallel_size based on your setup\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"wikipedia\", language=\"en\", date=\"20240401\", split='train[:5%]', trust_remote_code=True)\n",
    "    texts = dataset['text']  # Extract the texts from the dataset\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = Tokenizer(model_path='cl100k_base.tiktoken')\n",
    "\n",
    "    # Prepare the dataset and dataloader\n",
    "    seq_len = 2048\n",
    "    wiki_dataset = WikipediaDataset(texts, tokenizer, seq_len)\n",
    "    dataloader = DataLoader(wiki_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Initialize the model\n",
    "    model_args = ModelArgs(\n",
    "        vocab_size=tokenizer.get_vocab_size(),\n",
    "        dim=512,\n",
    "        n_layers=6,\n",
    "        n_heads=8,\n",
    "        ffn_dim_multiplier=4\n",
    "    )\n",
    "\n",
    "    model = Transformer(model_args).cuda()\n",
    "\n",
    "    # Define optimizer and learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "    num_epochs = 3\n",
    "    total_steps = len(dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    # Mixed precision scaler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Training loop with gradient accumulation and mixed precision\n",
    "    gradient_accumulation_steps = 4  # Accumulate gradients over 4 batches\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
    "            batch = batch.cuda()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(batch, start_pos=0)\n",
    "                logits = outputs.view(-1, outputs.size(-1))\n",
    "                targets = batch.view(-1)\n",
    "\n",
    "                loss = F.cross_entropy(logits, targets, ignore_index=tokenizer.pad_id)\n",
    "                loss = loss / gradient_accumulation_steps  # Scale the loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "    # Finalize model parallel\n",
    "    fs_init.destroy_model_parallel()\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
