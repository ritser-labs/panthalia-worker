{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (8.1.2)\n",
      "Requirement already satisfied: notebook in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from jupyter) (6.5.4)\n",
      "Requirement already satisfied: qtconsole in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from jupyter) (5.4.2)\n",
      "Requirement already satisfied: jupyter-console in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from jupyter) (6.5.4)\n",
      "Requirement already satisfied: ipykernel in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from jupyter) (6.25.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: backcall in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (7.4.9)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (5.3.0)\n",
      "Requirement already satisfied: nest-asyncio in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (1.5.6)\n",
      "Requirement already satisfied: packaging in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (23.1)\n",
      "Requirement already satisfied: psutil in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=20 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (23.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (6.3.2)\n",
      "Requirement already satisfied: lxml in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (4.9.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (4.12.2)\n",
      "Requirement already satisfied: bleach in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (0.4)\n",
      "Requirement already satisfied: jinja2>=3.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (3.1.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (2.1.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (0.5.13)\n",
      "Requirement already satisfied: nbformat>=5.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (5.9.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (1.2.1)\n",
      "Requirement already satisfied: argon2-cffi in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from notebook->jupyter) (21.3.0)\n",
      "Requirement already satisfied: ipython-genutils in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from notebook->jupyter) (0.2.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from notebook->jupyter) (0.17.1)\n",
      "Requirement already satisfied: prometheus-client in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from notebook->jupyter) (0.14.1)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from notebook->jupyter) (0.5.5)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from qtconsole->jupyter) (2.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (3.10.0)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbclassic>=0.4.7->notebook->jupyter) (1.23.4)\n",
      "Requirement already satisfied: notebook-shim>=0.1.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbclassic>=0.4.7->notebook->jupyter) (0.2.2)\n",
      "Requirement already satisfied: fastjsonschema in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbformat>=5.1->nbconvert->jupyter) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from nbformat>=5.1->nbconvert->jupyter) (4.17.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from bleach->nbconvert->jupyter) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: executing in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (3.5.0)\n",
      "Requirement already satisfied: websocket-client in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (0.58.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.15.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (1.2.0)\n",
      "Requirement already satisfied: pycparser in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: fairscale in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (0.4.13)\n",
      "Requirement already satisfied: tiktoken==0.4.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: fair in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: blobfile in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: datasets in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (2.12.0)\n",
      "Requirement already satisfied: mwparserfromhell in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (0.6.6)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from tiktoken==0.4.0) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from tiktoken==0.4.0) (2.31.0)\n",
      "Requirement already satisfied: filelock in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from fairscale) (1.24.3)\n",
      "Requirement already satisfied: matplotlib in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from fair) (3.7.2)\n",
      "Requirement already satisfied: pandas in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from fair) (2.0.3)\n",
      "Requirement already satisfied: pooch in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from fair) (1.8.1)\n",
      "Requirement already satisfied: scipy in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from fair) (1.11.1)\n",
      "Requirement already satisfied: tqdm in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from fair) (4.65.0)\n",
      "Requirement already satisfied: xarray in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from fair) (2023.6.0)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from blobfile) (3.20.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from blobfile) (1.26.16)\n",
      "Requirement already satisfied: lxml~=4.9 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from blobfile) (4.9.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (2023.11.17)\n",
      "Requirement already satisfied: six in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from matplotlib->fair) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from matplotlib->fair) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from matplotlib->fair) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from matplotlib->fair) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from matplotlib->fair) (10.0.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from matplotlib->fair) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from matplotlib->fair) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from pandas->fair) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from pandas->fair) (2023.3)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from pooch->fair) (3.10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ibrahim/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade jupyter ipywidgets\n",
    "%pip install torch fairscale tiktoken==0.4.0 fair blobfile datasets mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Layer state dict saved to data/layer_0.pt\n",
      "Layer state dict saved to data/layer_1.pt\n",
      "Layer state dict saved to data/layer_2.pt\n",
      "Layer state dict saved to data/layer_3.pt\n",
      "Layer state dict saved to data/embedding.pt\n",
      "Layer state dict saved to data/output.pt\n",
      "Saved to data/freqs_cis.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 0it [00:00, ?it/s]INFO:datasets_modules.datasets.wikipedia.d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001.wikipedia:generating examples from = https://dumps.wikimedia.org/enwiki/20240401/enwiki-20240401-pages-articles-multistream1.xml-p1p41242.bz2\n",
      "INFO:datasets_modules.datasets.wikipedia.d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001.wikipedia:generating examples from = https://dumps.wikimedia.org/enwiki/20240401/enwiki-20240401-pages-articles-multistream3.xml-p151574p311329.bz2\n",
      "INFO:datasets_modules.datasets.wikipedia.d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001.wikipedia:generating examples from = https://dumps.wikimedia.org/enwiki/20240401/enwiki-20240401-pages-articles-multistream4.xml-p311330p558391.bz2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting content from https://dumps.wikimedia.org/enwiki/20240401/enwiki-20240401-pages-articles-multistream1.xml-p1p41242.bz2\n",
      "Extracting content from"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets_modules.datasets.wikipedia.d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001.wikipedia:generating examples from = https://dumps.wikimedia.org/enwiki/20240401/enwiki-20240401-pages-articles-multistream2.xml-p41243p151573.bz2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting content from"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets_modules.datasets.wikipedia.d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001.wikipedia:generating examples from = https://dumps.wikimedia.org/enwiki/20240401/enwiki-20240401-pages-articles-multistream5.xml-p558392p958045.bz2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracting content fromExtracting content fromhttps://dumps.wikimedia.org/enwiki/20240401/enwiki-20240401-pages-articles-multistream3.xml-p151574p311329.bz2https://dumps.wikimedia.org/enwiki/20240401/enwiki-20240401-pages-articles-multistream4.xml-p311330p558391.bz2  \n",
      "\n",
      "https://dumps.wikimedia.org/enwiki/20240401/enwiki-20240401-pages-articles-multistream2.xml-p41243p151573.bz2https://dumps.wikimedia.org/enwiki/20240401/enwiki-20240401-pages-articles-multistream5.xml-p558392p958045.bz2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets_modules.datasets.wikipedia.d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001.wikipedia:generating examples from = https://dumps.wikimedia.org/enwiki/20240401/enwiki-20240401-pages-articles-multistream6.xml-p958046p1483661.bz2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting content from https://dumps.wikimedia.org/enwiki/20240401/enwiki-20240401-pages-articles-multistream6.xml-p958046p1483661.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got disconnected from remote data host. Retrying in 5sec [1/20]\n",
      "Got disconnected from remote data host. Retrying in 5sec [1/20]\n",
      "WARNING:datasets.download.streaming_download_manager:Got disconnected from remote data host. Retrying in 5sec [1/20]\n",
      "WARNING:datasets.download.streaming_download_manager:Got disconnected from remote data host. Retrying in 5sec [1/20]\n",
      "Got disconnected from remote data host. Retrying in 5sec [1/20]\n",
      "WARNING:datasets.download.streaming_download_manager:Got disconnected from remote data host. Retrying in 5sec [1/20]\n",
      "INFO:root:Batch (token IDs): torch.Size([2, 2048])\n",
      "INFO:root:Targets: torch.Size([2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input: <bAnarchism is a political philosophy and movement that is against all forms of authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy, typically including the state and capitalism. Anarchism advocates for the replacement of the state with stateless societies and voluntary free associations. As a historically left-wing movement, this reading of anarchism is placed on the farthest left of the political spectrum, usually described as the libertarian wing of the socialist movement (libertarian socialism).\n",
      "\n",
      "Although traces of anarchist ideas are found all throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers' struggles for emancipation. Various anarchist schools of thought formed during this period. Anarchists have taken part in several revolutions, most notably in the Paris Commune, the Russian Civil War and the Spanish Civil War, whose end marked the end of the classical era of anarchism. In the last decades of the 20th and into the 21st century, the anarchist movement has been resurgent once more, growing in popularity and influence within anti-capitalist, anti-war and anti-globalisation movements.\n",
      "\n",
      "Anarchists employ diverse approaches, which may be generally divided into revolutionary and evolutionary strategies; there is significant overlap between the two. Evolutionary methods try to simulate what an anarchist society might be like, but revolutionary tactics, which have historically taken a violent turn, aim to overthrow authority and the state. Many facets of human civilization have been influenced by anarchist theory, critique, and praxis.\n",
      "\n",
      "Etymology, terminology, and definition \n",
      "\n",
      "The etymological origin of anarchism is from the Ancient Greek anarkhia (ἀναρχία), meaning \"without a ruler\", composed of the prefix an- (\"without\") and the word arkhos (\"leader\" or \"ruler\"). The suffix -ism denotes the ideological current that favours anarchy. Anarchism appears in English from 1642 as anarchisme and anarchy from 1539; early English usages emphasised a sense of disorder. Various factions within the French Revolution labelled their opponents as anarchists, although few such accused shared many views with later anarchists. Many revolutionaries of the 19th century such as William Godwin (1756–1836) and Wilhelm Weitling (1808–1871) would contribute to the anarchist doctrines of the next generation but did not use anarchist or anarchism in describing themselves or their beliefs.\n",
      "\n",
      "The first political philosopher to call himself an anarchist () was Pierre-Joseph Proudhon (1809–1865), marking the formal birth of anarchism in the mid-19th century. Since the 1890s and beginning in France, libertarianism has often been used as a synonym for anarchism and its use as a synonym is still common outside the United States. Some usages of libertarianism refer to individualistic free-market philosophy only, and free-market anarchism in particular is termed libertarian anarchism.\n",
      "\n",
      "While the term libertarian has been largely synonymous with anarchism, its meaning has more recently been diluted by wider adoption from ideologically disparate groups, including both the New Left and libertarian Marxists, who do not associate themselves with authoritarian socialists or a vanguard party, and extreme cultural liberals, who are primarily concerned with civil liberties. Additionally, some anarchists use libertarian socialist to avoid anarchism's negative connotations and emphasise its connections with socialism. Anarchism is broadly used to describe the anti-authoritarian wing of the socialist movement. Anarchism is contrasted to socialist forms which are state-oriented or from above. Scholars of anarchism generally highlight anarchism's socialist credentials and criticise attempts at creating dichotomies between the two. Some scholars describe anarchism as having many influences from liberalism, and being both liberal and socialist but more so. Many scholars reject anarcho-capitalism as a misunderstanding of anarchist principles.\n",
      "\n",
      "While opposition to the state is central to anarchist thought, defining anarch<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p\n",
      "Expected next token: ism\n",
      "Saved to data/inputs.pt\n",
      "Saved to data/targets.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/inputs.pt\n",
      "Layer state dict loaded from data/embedding.pt\n",
      "Saved to data/inputs_embed.pt\n",
      "\n",
      "INFO:root:Embedded inputs: torch.Size([2, 2048, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/inputs_embed.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Saved to data/mask.pt\n",
      "Saved to data/inputs_layer_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/inputs_layer_0.pt\n",
      "Layer state dict loaded from data/layer_0.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Loaded from data/mask.pt\n",
      "Saved to data/logits_layer_0.pt\n",
      "\n",
      "INFO:root:Inputs after layer 0: torch.Size([2, 2048, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/logits_layer_0.pt\n",
      "Saved to data/inputs_layer_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/inputs_layer_1.pt\n",
      "Layer state dict loaded from data/layer_1.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Loaded from data/mask.pt\n",
      "Saved to data/logits_layer_1.pt\n",
      "\n",
      "INFO:root:Inputs after layer 1: torch.Size([2, 2048, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/logits_layer_1.pt\n",
      "Saved to data/inputs_layer_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/inputs_layer_2.pt\n",
      "Layer state dict loaded from data/layer_2.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Loaded from data/mask.pt\n",
      "Saved to data/logits_layer_2.pt\n",
      "\n",
      "INFO:root:Inputs after layer 2: torch.Size([2, 2048, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/logits_layer_2.pt\n",
      "Saved to data/inputs_layer_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/inputs_layer_3.pt\n",
      "Layer state dict loaded from data/layer_3.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Loaded from data/mask.pt\n",
      "Saved to data/logits_layer_3.pt\n",
      "\n",
      "INFO:root:Inputs after layer 3: torch.Size([2, 2048, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/logits_layer_3.pt\n",
      "Saved to data/final_inputs.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/final_inputs.pt\n",
      "Layer state dict loaded from data/output.pt\n",
      "Saved to data/logits.pt\n",
      "\n",
      "INFO:root:Final logits: torch.Size([2, 2048, 100277])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/logits.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/logits.pt\n",
      "Loaded from data/targets.pt\n",
      "Saved to data/loss.pt\n",
      "Saved to data/logits_grad.pt\n",
      "\n",
      "INFO:root:Loss: 11.4862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/loss.pt\n",
      "Loss: 11.4862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Logits gradients: torch.Size([2, 2048, 100277])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/logits_grad.pt\n",
      "Saved to data/final_error.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/final_inputs.pt\n",
      "Loaded from data/final_error.pt\n",
      "Layer state dict loaded from data/output.pt\n",
      "Saved to data/error_output_final_logits.pt\n",
      "\n",
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/error_output_final_logits.pt\n",
      "Loaded from data/inputs_layer_3.pt\n",
      "Layer state dict loaded from data/layer_3.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Loaded from data/mask.pt\n",
      "Saved to data/error_layer_3.pt\n",
      "\n",
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/error_layer_3.pt\n",
      "Loaded from data/inputs_layer_2.pt\n",
      "Layer state dict loaded from data/layer_2.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Loaded from data/mask.pt\n",
      "Saved to data/error_layer_2.pt\n",
      "\n",
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/error_layer_2.pt\n",
      "Loaded from data/inputs_layer_1.pt\n",
      "Layer state dict loaded from data/layer_1.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Loaded from data/mask.pt\n",
      "Saved to data/error_layer_1.pt\n",
      "\n",
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/error_layer_1.pt\n",
      "Loaded from data/inputs_layer_0.pt\n",
      "Layer state dict loaded from data/layer_0.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Loaded from data/mask.pt\n",
      "Saved to data/error_layer_0.pt\n",
      "\n",
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/error_layer_0.pt\n",
      "Loaded from data/inputs.pt\n",
      "Layer state dict loaded from data/embedding.pt\n",
      "Saved to data/error_output_embedding.pt\n",
      "\n",
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/error_layer_0.pt\n",
      "Layer state dict loaded from data/layer_0.pt\n",
      "File data/adam_m_0.pt does not exist\n",
      "File data/adam_v_0.pt does not exist\n",
      "Layer state dict saved to data/layer_0.pt\n",
      "Saved to data/adam_m_0.pt\n",
      "Saved to data/adam_v_0.pt\n",
      "\n",
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/error_layer_1.pt\n",
      "Layer state dict loaded from data/layer_1.pt\n",
      "File data/adam_m_1.pt does not exist\n",
      "File data/adam_v_1.pt does not exist\n",
      "Layer state dict saved to data/layer_1.pt\n",
      "Saved to data/adam_m_1.pt\n",
      "Saved to data/adam_v_1.pt\n",
      "\n",
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/error_layer_2.pt\n",
      "Layer state dict loaded from data/layer_2.pt\n",
      "File data/adam_m_2.pt does not exist\n",
      "File data/adam_v_2.pt does not exist\n",
      "Layer state dict saved to data/layer_2.pt\n",
      "Saved to data/adam_m_2.pt\n",
      "Saved to data/adam_v_2.pt\n",
      "\n",
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/error_layer_3.pt\n",
      "Layer state dict loaded from data/layer_3.pt\n",
      "File data/adam_m_3.pt does not exist\n",
      "File data/adam_v_3.pt does not exist\n",
      "Layer state dict saved to data/layer_3.pt\n",
      "Saved to data/adam_m_3.pt\n",
      "Saved to data/adam_v_3.pt\n",
      "\n",
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/error_output_final_logits.pt\n",
      "Layer state dict loaded from data/output.pt\n",
      "File data/adam_m_output.pt does not exist\n",
      "File data/adam_v_output.pt does not exist\n",
      "Layer state dict saved to data/output.pt\n",
      "Saved to data/adam_m_output.pt\n",
      "Saved to data/adam_v_output.pt\n",
      "\n",
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/error_output_embedding.pt\n",
      "Layer state dict loaded from data/embedding.pt\n",
      "File data/adam_m_embedding.pt does not exist\n",
      "File data/adam_v_embedding.pt does not exist\n",
      "Layer state dict saved to data/embedding.pt\n",
      "Saved to data/adam_m_embedding.pt\n",
      "Saved to data/adam_v_embedding.pt\n",
      "\n",
      "Epoch 1/3: 1it [02:16, 136.54s/it]INFO:root:Batch (token IDs): torch.Size([2, 2048])\n",
      "INFO:root:Targets: torch.Size([2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input: <bA hotline is a point-to-point communications link in which a call is automatically directed to the preselected destination without any additional action by the user when the end instrument goes off-hook. An example would be a phone that automatically connects to emergency services on picking up the receiver. Therefore, dedicated hotline phones do not need a rotary dial or keypad. A hotline can also be called an automatic signaling, ringdown, or off-hook service.\n",
      "\n",
      "For crises and service \n",
      "True hotlines cannot be used to originate calls other than to preselected destinations.  However, in common or colloquial usage, a \"hotline\" often refers to a call center reachable by dialing a standard telephone number, or sometimes the phone numbers themselves.\n",
      "\n",
      "This is especially the case with 24-hour, noncommercial numbers, such as police tip hotlines or suicide crisis hotlines, which are staffed around the clock and thereby give the appearance of real hotlines.  Increasingly, however, the term is found being applied to any customer service telephone number.\n",
      "\n",
      "Between states\n",
      "\n",
      "Russia–United States \n",
      "\n",
      "The most famous hotline between states is the Moscow–Washington hotline, also known as the \"red telephone\", although telephones have never been used in this capacity. This direct communications link was established on June 20, 1963, in the wake of the Cuban Missile Crisis, which convinced both sides of the need for better communications. It was the first time used by U.S. President John F. Kennedy on August 30, 1963 and utilized teletypewriter technology, later replaced by telecopier and then by electronic mail.\n",
      "\n",
      "United Kingdom–United States \n",
      "\n",
      "Already during World War II—two decades before the Washington–Moscow hotline was established—there was a hotline between No. 10 Downing Street and the Cabinet War Room bunker under the Treasury, Whitehall; with the White House in Washington, D.C. From 1943 to 1946, this link was made secure by using the very first voice encryption machine, called SIGSALY.\n",
      "\n",
      "China–Russia \n",
      "A hotline connection between Beijing and Moscow was used during the 1969 frontier confrontation between the two countries. The Chinese however refused the Russian peace attempts and ended the communications link. After a reconciliation between the former enemies, the hotline between China and Russia was revived in 1996.\n",
      "\n",
      "France–Russia \n",
      "On his visit to the Soviet Union in 1966, French President Charles de Gaulle announced that a hotline would be established between Paris and Moscow. The line was upgraded from a telex to a high-speed fax machine in 1989.\n",
      "\n",
      "Russia–United Kingdom \n",
      "A London–Moscow hotline was not formally established until a treaty of friendship between the two countries in 1992. An upgrade was announced when Foreign Secretary William Hague visited Moscow in 2011.\n",
      "\n",
      "India–Pakistan \n",
      "On 20 June 2004, both India and Pakistan agreed to extend a nuclear testing ban and to set up an Islamabad–New Delhi hotline between their foreign secretaries aimed at preventing misunderstandings that might lead to nuclear war. The hotline was set up with the assistance of United States military officers.\n",
      "\n",
      "China–United States \n",
      "\n",
      "The United States and China set up a defense hotline in 2008, but it has rarely been used in crises.\n",
      "\n",
      "China–India \n",
      "India and China announced a hotline for the foreign ministers of both countries while reiterating their commitment to strengthening ties and building \"mutual political trust\". As of August 2015 the hotline was yet to be made operational.\n",
      "\n",
      "China–Japan \n",
      "In February 2013, the Senkaku Islands dispute gave renewed impetus to a China–Japan hotline, which had been agreed to but due to rising tensions had not been established.\n",
      "\n",
      "North and South Korea \n",
      "Between North and South Korea there are over 40 direct phone lines, the first of which was opened in September 1971. Most of these hotlines run through the Panmunjeom Joint Security Area (JSA) and are maintained by the Red Cross. Since 1971, North Korea has deactivated the hotlines seven times, the last time in February 2016. After Kim Jong-un's New Years address, the border hotline was reopened on January 3, 2018.\n",
      "\n",
      "India–United States \n",
      "In August 2015 the hotline between the White House and New Delhi became operational. The decision of establishing this hotline was taken during Obama's visit to India in January 2015. This is the first hotline connecting an Indian Prime Minister to a head of state.\n",
      "\n",
      "See also \n",
      " Bat phone\n",
      " Complaint system\n",
      "\n",
      "References\n",
      "\n",
      "External links \n",
      " Top Level Telecommunications: Bilateral Hotlines Worldwide\n",
      "\n",
      "Telecommunication services\n",
      "Bilateral relations\n",
      "Hotline between countries\n",
      "\n",
      "de:Heißer Draht<<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p<p\n",
      "Expected next token: <p\n",
      "Saved to data/inputs.pt\n",
      "Saved to data/targets.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/inputs.pt\n",
      "Layer state dict loaded from data/embedding.pt\n",
      "Saved to data/inputs_embed.pt\n",
      "\n",
      "INFO:root:Embedded inputs: torch.Size([2, 2048, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/inputs_embed.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Saved to data/mask.pt\n",
      "Saved to data/inputs_layer_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/inputs_layer_0.pt\n",
      "Layer state dict loaded from data/layer_0.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Loaded from data/mask.pt\n",
      "Saved to data/logits_layer_0.pt\n",
      "\n",
      "INFO:root:Inputs after layer 0: torch.Size([2, 2048, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/logits_layer_0.pt\n",
      "Saved to data/inputs_layer_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/inputs_layer_1.pt\n",
      "Layer state dict loaded from data/layer_1.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Loaded from data/mask.pt\n",
      "Saved to data/logits_layer_1.pt\n",
      "\n",
      "INFO:root:Inputs after layer 1: torch.Size([2, 2048, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/logits_layer_1.pt\n",
      "Saved to data/inputs_layer_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/inputs_layer_2.pt\n",
      "Layer state dict loaded from data/layer_2.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Loaded from data/mask.pt\n",
      "Saved to data/logits_layer_2.pt\n",
      "\n",
      "INFO:root:Inputs after layer 2: torch.Size([2, 2048, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/logits_layer_2.pt\n",
      "Saved to data/inputs_layer_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/inputs_layer_3.pt\n",
      "Layer state dict loaded from data/layer_3.pt\n",
      "Loaded from data/freqs_cis.pt\n",
      "Loaded from data/mask.pt\n",
      "Saved to data/logits_layer_3.pt\n",
      "\n",
      "INFO:root:Inputs after layer 3: torch.Size([2, 2048, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/logits_layer_3.pt\n",
      "Saved to data/final_inputs.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/final_inputs.pt\n",
      "Layer state dict loaded from data/output.pt\n",
      "Saved to data/logits.pt\n",
      "\n",
      "INFO:root:Final logits: torch.Size([2, 2048, 100277])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/logits.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded from data/logits.pt\n",
      "Loaded from data/targets.pt\n",
      "Saved to data/loss.pt\n",
      "Saved to data/logits_grad.pt\n",
      "\n",
      "INFO:root:Loss: 11.5948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/loss.pt\n",
      "Loss: 11.5948\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import subprocess\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from common import save_to_disk, load_from_disk, save_layer_state_dict, load_layer_state_dict, model_args, tokenizer\n",
    "from tokenizer import Tokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from model import TransformerBlock, VocabParallelEmbedding, ColumnParallelLinear, RMSNorm, Transformer, precompute_freqs_cis\n",
    "from fairscale.nn.model_parallel.initialize import initialize_model_parallel, model_parallel_is_initialized\n",
    "from fairscale.nn.model_parallel.layers import (\n",
    "    ColumnParallelLinear,\n",
    "    RowParallelLinear,\n",
    "    VocabParallelEmbedding,\n",
    ")\n",
    "import random\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "MAX_SEQ_LEN = model_args.max_seq_len\n",
    "\n",
    "class StreamingWikipediaDataset(IterableDataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def parse(self, text):\n",
    "        tokens = self.tokenizer.encode(text, bos=True, eos=True)\n",
    "        seq_len = random.randint(1, self.max_seq_len)\n",
    "        if len(tokens) < seq_len + 1:\n",
    "            tokens = tokens + [self.tokenizer.pad_id] * (seq_len + 1 - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:seq_len + 1]\n",
    "        return torch.tensor(tokens)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in self.texts:\n",
    "            tokens = self.parse(item['text'])\n",
    "            yield tokens[:-1], tokens[-1]  # Inputs and the next token\n",
    "\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    pad_id = tokenizer.pad_id\n",
    "    inputs = [torch.cat([item[0], torch.tensor([pad_id] * (MAX_SEQ_LEN - len(item[0])))]) for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    inputs = torch.stack(inputs)\n",
    "    targets = torch.tensor(targets)\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def wait_for_file(filename, timeout=30):\n",
    "    start_time = time.time()\n",
    "    while not os.path.exists(filename):\n",
    "        if time.time() - start_time > timeout:\n",
    "            raise TimeoutError(f\"Timeout waiting for {filename}\")\n",
    "        time.sleep(0.1)\n",
    "\n",
    "\n",
    "def initialize_layers(model_args):\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, (nn.Linear, ColumnParallelLinear, RowParallelLinear, VocabParallelEmbedding)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    for layer_idx in range(model_args.n_layers):\n",
    "        layer = TransformerBlock(layer_idx, model_args)\n",
    "        layer.apply(init_weights)\n",
    "\n",
    "        for param in layer.parameters():\n",
    "            if torch.isnan(param).any() or torch.isinf(param).any():\n",
    "                raise ValueError(f\"NaNs or Infs detected in weights of layer {layer_idx}\")\n",
    "\n",
    "        save_layer_state_dict(layer.state_dict(), f\"data/layer_{layer_idx}.pt\")\n",
    "\n",
    "    embedding = VocabParallelEmbedding(model_args.vocab_size, model_args.dim)\n",
    "    embedding.apply(init_weights)\n",
    "\n",
    "    for param in embedding.parameters():\n",
    "        if torch.isnan(param).any() or torch.isinf(param).any():\n",
    "            raise ValueError(\"NaNs or Infs detected in embedding weights\")\n",
    "\n",
    "    save_layer_state_dict(embedding.state_dict(), \"data/embedding.pt\")\n",
    "\n",
    "    output = ColumnParallelLinear(model_args.dim, model_args.vocab_size, bias=False)\n",
    "    output.apply(init_weights)\n",
    "\n",
    "    for param in output.parameters():\n",
    "        if torch.isnan(param).any() or torch.isinf(param).any():\n",
    "            raise ValueError(\"NaNs or Infs detected in output weights\")\n",
    "\n",
    "    save_layer_state_dict(output.state_dict(), \"data/output.pt\")\n",
    "\n",
    "    freqs_cis = precompute_freqs_cis(\n",
    "        model_args.dim // model_args.n_heads,\n",
    "        model_args.max_seq_len * 2,\n",
    "        model_args.rope_theta,\n",
    "    )\n",
    "    save_to_disk(freqs_cis, \"data/freqs_cis.pt\")\n",
    "\n",
    "def process_batch(batch, tokenizer, epoch_loss, learning_rate, beta1, beta2, epsilon, weight_decay, t):\n",
    "    try:\n",
    "        inputs, targets = batch\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if torch.isnan(inputs).any() or torch.isinf(inputs).any():\n",
    "            raise ValueError(\"NaNs or Infs detected in input batch\")\n",
    "\n",
    "        if torch.isnan(targets).any() or torch.isinf(targets).any():\n",
    "            raise ValueError(\"NaNs or Infs detected in target batch\")\n",
    "\n",
    "        logging.info(f\"Batch (token IDs): {inputs.shape}\")\n",
    "        logging.info(f\"Targets: {targets.shape}\")\n",
    "\n",
    "        # Print the actual sample input and target\n",
    "        input_text = tokenizer.decode(inputs[0].tolist())\n",
    "        target_text = tokenizer.decode([targets[0].item()])\n",
    "        print(f\"Sample input: {input_text}\")\n",
    "        print(f\"Expected next token: {target_text}\")\n",
    "\n",
    "        # Ensure targets tensor has correct shape [batch_size, seq_len]\n",
    "        if targets.dim() == 1:\n",
    "            targets = targets.unsqueeze(1)  # Add dimension if missing\n",
    "\n",
    "        if targets.shape[1] != inputs.shape[1]:\n",
    "            targets = targets.expand(inputs.shape[0], inputs.shape[1])\n",
    "\n",
    "        save_to_disk(inputs, \"data/inputs.pt\")\n",
    "        save_to_disk(targets, \"data/targets.pt\")\n",
    "\n",
    "        run_command(f\"python3 worker.py --task embed --batch data/inputs.pt --embedding_file data/embedding.pt --outputs data/inputs_embed.pt\")\n",
    "        wait_for_file(\"data/inputs_embed.pt\")\n",
    "        inputs_embed = load_from_disk(\"data/inputs_embed.pt\")\n",
    "        \n",
    "\n",
    "        logging.info(f\"Embedded inputs: {inputs_embed.shape}\")\n",
    "\n",
    "        check_for_nans(inputs_embed, \"inputs after embedding\")\n",
    "\n",
    "        freqs_cis = load_from_disk(\"data/freqs_cis.pt\")\n",
    "\n",
    "        seqlen = inputs_embed.shape[1]\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.triu(torch.full((seqlen, seqlen), float('-inf')), diagonal=1).to(device)\n",
    "            save_to_disk(mask, \"data/mask.pt\")\n",
    "\n",
    "        for layer_idx in range(model_args.n_layers):\n",
    "            inputs_file = f\"data/inputs_layer_{layer_idx}.pt\"\n",
    "            logits_file = f\"data/logits_layer_{layer_idx}.pt\"\n",
    "            state_dict_file = f\"data/layer_{layer_idx}.pt\"\n",
    "            save_to_disk(inputs_embed, inputs_file)\n",
    "            run_command(f\"python3 worker.py --task forward --layer_idx {layer_idx} --inputs {inputs_file} --state_dict {state_dict_file} --freqs_cis data/freqs_cis.pt --outputs {logits_file} --mask data/mask.pt\")\n",
    "            wait_for_file(logits_file)\n",
    "            inputs_embed, inputs, layer = load_from_disk(logits_file)\n",
    "\n",
    "            logging.info(f\"Inputs after layer {layer_idx}: {inputs_embed.shape}\")\n",
    "\n",
    "            if torch.isnan(inputs_embed).any() or torch.isinf(inputs_embed).any():\n",
    "                raise ValueError(f\"NaNs or Infs detected in inputs after layer {layer_idx}\")\n",
    "\n",
    "            check_for_nans(inputs_embed, f\"inputs after layer {layer_idx}\")\n",
    "\n",
    "        norm = RMSNorm(model_args.dim, eps=model_args.norm_eps).to(device)\n",
    "        inputs_embed = norm(inputs_embed)\n",
    "        save_to_disk(inputs_embed, \"data/final_inputs.pt\")\n",
    "\n",
    "        run_command(f\"python3 worker.py --task final_logits --inputs data/final_inputs.pt --state_dict data/output.pt --logits_file data/logits.pt\")\n",
    "        wait_for_file(\"data/logits.pt\")\n",
    "        logits, inputs_embed, output_layer = load_from_disk(\"data/logits.pt\")\n",
    "\n",
    "        logging.info(f\"Final logits: {logits.shape}\")\n",
    "\n",
    "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "            raise ValueError(\"NaNs or Infs detected in final logits\")\n",
    "\n",
    "        check_for_nans(logits, \"final logits\")\n",
    "\n",
    "        run_command(f\"python3 worker.py --task loss --logits data/logits.pt --targets data/targets.pt --loss_file data/loss.pt --logits_grad_file data/logits_grad.pt\")\n",
    "        wait_for_file(\"data/loss.pt\")\n",
    "        wait_for_file(\"data/logits_grad.pt\")\n",
    "\n",
    "        loss = load_from_disk(\"data/loss.pt\")\n",
    "        logging.info(f\"Loss: {loss:.4f}\")\n",
    "        print(f\"Loss: {loss:.4f}\")\n",
    "        epoch_loss += loss\n",
    "        logits_grad, inputs, output_layer = load_from_disk(\"data/logits_grad.pt\")\n",
    "\n",
    "        logging.info(f\"Logits gradients: {logits_grad.shape}\")\n",
    "\n",
    "        save_to_disk(logits_grad, \"data/final_error.pt\")\n",
    "        final_logits_error_file = \"data/error_output_final_logits.pt\"\n",
    "        run_command(f\"python3 worker.py --task final_logits_backward --error data/final_error.pt --state_dict data/output.pt --inputs data/final_inputs.pt --error_output_file {final_logits_error_file}\")\n",
    "\n",
    "        # Backpropagate through the transformer layers\n",
    "        for layer_idx in reversed(range(model_args.n_layers)):\n",
    "            inputs_file = f\"data/inputs_layer_{layer_idx}.pt\"\n",
    "            state_dict_file = f\"data/layer_{layer_idx}.pt\"\n",
    "            error_output_file = f\"data/error_layer_{layer_idx}.pt\"\n",
    "            if layer_idx == model_args.n_layers - 1:\n",
    "                error_file = final_logits_error_file\n",
    "            else:\n",
    "                error_file = f\"data/error_layer_{layer_idx + 1}.pt\"\n",
    "\n",
    "            run_command(f\"python3 worker.py --task backward --inputs {inputs_file} --layer_idx {layer_idx} --error {error_file} --state_dict {state_dict_file} --error_output_file {error_output_file} --freqs_cis data/freqs_cis.pt --mask data/mask.pt\")\n",
    "\n",
    "            wait_for_file(error_output_file)\n",
    "\n",
    "        # Backpropagate through the embedding layer\n",
    "        embed_error_file = \"data/error_output_embedding.pt\"\n",
    "        run_command(f\"python3 worker.py --task embed_backward --error data/error_layer_0.pt --batch data/inputs.pt --embedding_file data/embedding.pt --error_output_file {embed_error_file}\")\n",
    "\n",
    "        # Update weights using AdamW optimizer\n",
    "        for layer_idx in range(model_args.n_layers):\n",
    "            error_output_file = f\"data/error_layer_{layer_idx}.pt\"\n",
    "            run_command(f\"python3 worker.py --task apply_adamw --layer_idx {layer_idx} --grads {error_output_file} --learning_rate {learning_rate} --beta1 {beta1} --beta2 {beta2} --epsilon {epsilon} --weight_decay {weight_decay} --t {t}\")\n",
    "\n",
    "        # Update weights for embedding and output layers\n",
    "        run_command(f\"python3 worker.py --task apply_adamw --layer_idx -1 --grads {final_logits_error_file} --learning_rate {learning_rate} --beta1 {beta1} --beta2 {beta2} --epsilon {epsilon} --weight_decay {weight_decay} --t {t}\")\n",
    "        run_command(f\"python3 worker.py --task apply_adamw --layer_idx -2 --grads {embed_error_file} --learning_rate {learning_rate} --beta1 {beta1} --beta2 {beta2} --epsilon {epsilon} --weight_decay {weight_decay} --t {t}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process and save batch: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def run_command(command):\n",
    "    try:\n",
    "        result = subprocess.run(command, shell=True, text=True, capture_output=True, check=True)\n",
    "        logging.info(result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Command '{command}' failed with error: {e.stderr}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    os.environ['WORLD_SIZE'] = '1'\n",
    "    os.environ['RANK'] = '0'\n",
    "\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    initialize_model_parallel(model_parallel_size_=1)\n",
    "\n",
    "    dataset = load_dataset(\"wikipedia\", language=\"en\", date=\"20240401\", split='train', streaming=True, trust_remote_code=True)\n",
    "\n",
    "    max_seq_len = MAX_SEQ_LEN\n",
    "    batch_size = 2  # Adjust batch size here\n",
    "    wiki_dataset = StreamingWikipediaDataset(dataset, tokenizer, max_seq_len)\n",
    "    dataloader = DataLoader(wiki_dataset, batch_size=batch_size, shuffle=False, num_workers=6, collate_fn=pad_collate_fn)\n",
    "\n",
    "    num_epochs = 3\n",
    "    learning_rate = 1e-4\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    weight_decay = 1e-2\n",
    "\n",
    "    initialize_layers(model_args)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
    "            try:\n",
    "                process_batch(batch, tokenizer, epoch_loss, learning_rate, beta1, beta2, epsilon, weight_decay, step + 1)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing batch at step {step}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def check_for_nans(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaNs detected in {name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
