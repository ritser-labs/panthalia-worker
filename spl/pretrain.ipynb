{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tokenizer.__init__() got an unexpected keyword argument 'model_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m texts \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Initialize the tokenizer\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcl100k_base.tiktoken\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Prepare the dataset and dataloader\u001b[39;00m\n\u001b[1;32m     39\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Tokenizer.__init__() got an unexpected keyword argument 'model_path'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import Transformer, ModelArgs\n",
    "from tokenizer import Tokenizer\n",
    "\n",
    "class WikipediaDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokens = self.tokenizer.encode(text, bos=True, eos=True)\n",
    "        tokens = tokens[:self.seq_len] + [self.tokenizer.pad_id] * (self.seq_len - len(tokens))\n",
    "        return torch.tensor(tokens)\n",
    "\n",
    "def main():\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"wikipedia\", language=\"en\", date=\"20240401\", split='train[:5%]', trust_remote_code=True)\n",
    "    texts = dataset['text']\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = Tokenizer(encoding_name='cl100k_base')\n",
    "\n",
    "    # Prepare the dataset and dataloader\n",
    "    seq_len = 2048\n",
    "    wiki_dataset = WikipediaDataset(texts, tokenizer, seq_len)\n",
    "    dataloader = DataLoader(wiki_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Initialize the model\n",
    "    model_args = ModelArgs(\n",
    "        vocab_size=tokenizer.get_vocab_size(),\n",
    "        dim=512,\n",
    "        n_layers=6,\n",
    "        n_heads=8,\n",
    "        ffn_dim_multiplier=4\n",
    "    )\n",
    "\n",
    "    model = Transformer(model_args).cuda()\n",
    "\n",
    "    # Define optimizer and learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "    num_epochs = 3\n",
    "    total_steps = len(dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    # Mixed precision scaler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Training loop with gradient accumulation and mixed precision\n",
    "    gradient_accumulation_steps = 4\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
    "            batch = batch.cuda()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(batch, start_pos=0)\n",
    "                logits = outputs.view(-1, outputs.size(-1))\n",
    "                targets = batch.view(-1)\n",
    "\n",
    "                loss = F.cross_entropy(logits, targets, ignore_index=tokenizer.pad_id)\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward(retain_graph=(step + 1) % gradient_accumulation_steps != 0)\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
