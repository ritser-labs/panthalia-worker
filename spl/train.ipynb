{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "NUM_PROC = 24\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"wikipedia\", language=\"en\", date=\"20240401\", split='train[:5%]', trust_remote_code=True, num_proc=NUM_PROC)\n",
    "tokenizer_path = 'cl100k_base.tiktoken'  # Update this path to your tokenizer.json\n",
    "\n",
    "# Load your tokenizer\n",
    "from tokenizer import Tokenizer  # Assuming your tokenizer script is named tokenizer.py\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "# Function to tokenize the text\n",
    "def tokenize_function(examples):\n",
    "    # The tokenizer.encode function expects a string, so process each text entry in the batch\n",
    "    input_ids = [tokenizer.encode(text, bos=True, eos=True) for text in examples['text']]\n",
    "    return {'input_ids': input_ids}\n",
    "\n",
    "# Apply the tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=NUM_PROC)\n",
    "\n",
    "# Set the format of the dataset to torch\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids'])\n",
    "\n",
    "max_seq_len = 2048  # Define a maximum sequence length\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # Ensure tensors are detached from their computation graph\n",
    "    input_ids_list = [item['input_ids'].clone().detach().to(torch.long) for item in batch]\n",
    "\n",
    "    # Define a maximum sequence length\n",
    "    max_seq_len = 2048  # Adjust as necessary\n",
    "\n",
    "    # Pad or truncate all sequences to the same length\n",
    "    padded_or_truncated_input_ids = [\n",
    "        ids[:max_seq_len] if len(ids) > max_seq_len else F.pad(ids, (0, max_seq_len - len(ids)), value=tokenizer.pad_id)\n",
    "        for ids in input_ids_list\n",
    "    ]\n",
    "    padded_input_ids = pad_sequence(padded_or_truncated_input_ids, batch_first=True, padding_value=tokenizer.pad_id)\n",
    "    return {'input_ids': padded_input_ids}\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Model Layers and Parameters:\n",
      "tok_embeddings.weight, Shape: torch.Size([100512, 512]), Parameters: 51462144\n",
      "layers.0.attention.wq.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.0.attention.wk.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.0.attention.wv.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.0.attention.wo.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.0.feed_forward.w1.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.0.feed_forward.w2.weight, Shape: torch.Size([512, 5632]), Parameters: 2883584\n",
      "layers.0.feed_forward.w3.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.0.attention_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.0.ffn_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.1.attention.wq.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.1.attention.wk.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.1.attention.wv.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.1.attention.wo.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.1.feed_forward.w1.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.1.feed_forward.w2.weight, Shape: torch.Size([512, 5632]), Parameters: 2883584\n",
      "layers.1.feed_forward.w3.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.1.attention_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.1.ffn_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.2.attention.wq.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.2.attention.wk.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.2.attention.wv.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.2.attention.wo.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.2.feed_forward.w1.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.2.feed_forward.w2.weight, Shape: torch.Size([512, 5632]), Parameters: 2883584\n",
      "layers.2.feed_forward.w3.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.2.attention_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.2.ffn_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.3.attention.wq.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.3.attention.wk.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.3.attention.wv.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.3.attention.wo.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.3.feed_forward.w1.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.3.feed_forward.w2.weight, Shape: torch.Size([512, 5632]), Parameters: 2883584\n",
      "layers.3.feed_forward.w3.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.3.attention_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.3.ffn_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.4.attention.wq.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.4.attention.wk.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.4.attention.wv.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.4.attention.wo.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.4.feed_forward.w1.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.4.feed_forward.w2.weight, Shape: torch.Size([512, 5632]), Parameters: 2883584\n",
      "layers.4.feed_forward.w3.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.4.attention_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.4.ffn_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.5.attention.wq.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.5.attention.wk.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.5.attention.wv.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.5.attention.wo.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.5.feed_forward.w1.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.5.feed_forward.w2.weight, Shape: torch.Size([512, 5632]), Parameters: 2883584\n",
      "layers.5.feed_forward.w3.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.5.attention_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.5.ffn_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "output.weight, Shape: torch.Size([100512, 512]), Parameters: 51462144\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from fairscale.nn.model_parallel import initialize_model_parallel\n",
    "from model import Transformer, ModelArgs\n",
    "\n",
    "# Set up distributed environment\n",
    "def setup_distributed(world_size=1, rank=0):\n",
    "    if not dist.is_initialized():\n",
    "        dist.init_process_group(\n",
    "            backend='nccl',  # Use 'nccl' for GPUs, 'gloo' for CPU or multi-GPU setups\n",
    "            init_method='tcp://localhost:23456',  # Address for initializing communication\n",
    "            world_size=world_size,  # Total number of processes\n",
    "            rank=rank  # Rank of the current process\n",
    "        )\n",
    "        # Initialize model parallelism\n",
    "        model_parallel_size = 1  # Adjust this as per your setup\n",
    "        initialize_model_parallel(model_parallel_size)\n",
    "\n",
    "# Initialize distributed environment\n",
    "setup_distributed()\n",
    "\n",
    "\n",
    "# Now proceed to define your model parameters and create the model\n",
    "model_args = ModelArgs(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    dim=512,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    ffn_dim_multiplier=4\n",
    ")\n",
    "\n",
    "model = Transformer(model_args)\n",
    "\n",
    "\n",
    "\n",
    "def print_model_details(model):\n",
    "    total_params = 0\n",
    "    print(\"Model Layers and Parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "        print(f\"{name}, Shape: {param.size()}, Parameters: {param_count}\")\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "\n",
    "\n",
    "print_model_details(model)  # Call to print details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Call the model's forward method with the necessary parameters\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, start_pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Assuming that your model returns a dictionary with 'loss'\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Coding/ritser/magnum/spl/model.py:299\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, tokens, start_pos)\u001b[0m\n\u001b[1;32m    294\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack(\n\u001b[1;32m    295\u001b[0m         [torch\u001b[38;5;241m.\u001b[39mzeros((seqlen, start_pos), device\u001b[38;5;241m=\u001b[39mtokens\u001b[38;5;241m.\u001b[39mdevice), mask]\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mtype_as(h)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 299\u001b[0m     h \u001b[38;5;241m=\u001b[39m layer(h, start_pos, freqs_cis, mask)\n\u001b[1;32m    300\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(h)\n\u001b[1;32m    301\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(h)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Coding/ritser/magnum/spl/model.py:246\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    241\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m     mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    245\u001b[0m ):\n\u001b[0;32m--> 246\u001b[0m     h \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_norm(x), start_pos, freqs_cis, mask)\n\u001b[1;32m    247\u001b[0m     out \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_norm(h))\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Coding/ritser/magnum/spl/model.py:187\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     scores \u001b[38;5;241m=\u001b[39m scores \u001b[38;5;241m+\u001b[39m mask  \u001b[38;5;66;03m# (bs, n_local_heads, seqlen, cache_len + seqlen)\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m scores \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(scores\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(xq)\n\u001b[1;32m    188\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(scores, values)  \u001b[38;5;66;03m# (bs, n_local_heads, seqlen, head_dim)\u001b[39;00m\n\u001b[1;32m    189\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz, seqlen, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(1):  # For simplicity, assuming 1 epoch\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        \n",
    "        # Call the model's forward method with the necessary parameters\n",
    "        outputs = model(input_ids, start_pos=0)\n",
    "\n",
    "        # Assuming that your model returns a dictionary with 'loss'\n",
    "        loss = outputs['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Batch {i}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'llm_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
