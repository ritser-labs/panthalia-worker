{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "NUM_PROC = 24\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"wikipedia\", language=\"en\", date=\"20240401\", split='train[:5%]', trust_remote_code=True, num_proc=NUM_PROC)\n",
    "tokenizer_path = 'cl100k_base.tiktoken'  # Update this path to your tokenizer.json\n",
    "\n",
    "# Load your tokenizer\n",
    "from tokenizer import Tokenizer  # Assuming your tokenizer script is named tokenizer.py\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "# Function to tokenize the text\n",
    "def tokenize_function(examples):\n",
    "    # The tokenizer.encode function expects a string, so process each text entry in the batch\n",
    "    input_ids = [tokenizer.encode(text, bos=True, eos=True) for text in examples['text']]\n",
    "    return {'input_ids': input_ids}\n",
    "\n",
    "# Apply the tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=NUM_PROC)\n",
    "\n",
    "# Set the format of the dataset to torch\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids'])\n",
    "\n",
    "max_seq_len = 2048  # Define a maximum sequence length\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # Ensure tensors are detached from their computation graph\n",
    "    input_ids_list = [item['input_ids'].clone().detach().to(torch.long) for item in batch]\n",
    "\n",
    "    # Define a maximum sequence length\n",
    "    max_seq_len = 2048  # Adjust as necessary\n",
    "\n",
    "    # Pad or truncate all sequences to the same length\n",
    "    padded_or_truncated_input_ids = [\n",
    "        ids[:max_seq_len] if len(ids) > max_seq_len else F.pad(ids, (0, max_seq_len - len(ids)), value=tokenizer.pad_id)\n",
    "        for ids in input_ids_list\n",
    "    ]\n",
    "    padded_input_ids = pad_sequence(padded_or_truncated_input_ids, batch_first=True, padding_value=tokenizer.pad_id)\n",
    "    return {'input_ids': padded_input_ids}\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from fairscale.nn.model_parallel import initialize_model_parallel\n",
    "from model import Transformer, ModelArgs\n",
    "\n",
    "# Set up distributed environment\n",
    "def setup_distributed(world_size=1, rank=0):\n",
    "    if not dist.is_initialized():\n",
    "        dist.init_process_group(\n",
    "            backend='nccl',  # Use 'nccl' for GPUs, 'gloo' for CPU or multi-GPU setups\n",
    "            init_method='tcp://localhost:23456',  # Address for initializing communication\n",
    "            world_size=world_size,  # Total number of processes\n",
    "            rank=rank  # Rank of the current process\n",
    "        )\n",
    "        # Initialize model parallelism\n",
    "        model_parallel_size = 1  # Adjust this as per your setup\n",
    "        initialize_model_parallel(model_parallel_size)\n",
    "\n",
    "# Initialize distributed environment\n",
    "setup_distributed()\n",
    "\n",
    "\n",
    "# Now proceed to define your model parameters and create the model\n",
    "model_args = ModelArgs(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    dim=512,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    ffn_dim_multiplier=4\n",
    ")\n",
    "\n",
    "model = Transformer(model_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74836/2880490951.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_list = [torch.tensor(item['input_ids'], dtype=torch.long) for item in batch]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# For simplicity, assuming 1 epoch\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     14\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36mcollate_batch\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     31\u001b[0m input_ids_list \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Pad or truncate all sequences to the same length\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m padded_or_truncated_input_ids \u001b[38;5;241m=\u001b[39m [ids[:max_seq_len] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m>\u001b[39m max_seq_len \u001b[38;5;28;01melse\u001b[39;00m F\u001b[38;5;241m.\u001b[39mpad(ids, (\u001b[38;5;241m0\u001b[39m, max_seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(ids)), value\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_id) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m input_ids_list]\n\u001b[1;32m     34\u001b[0m padded_input_ids \u001b[38;5;241m=\u001b[39m pad_sequence(padded_or_truncated_input_ids, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_id)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: padded_input_ids}\n",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m input_ids_list \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Pad or truncate all sequences to the same length\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m padded_or_truncated_input_ids \u001b[38;5;241m=\u001b[39m [ids[:max_seq_len] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m>\u001b[39m max_seq_len \u001b[38;5;28;01melse\u001b[39;00m F\u001b[38;5;241m.\u001b[39mpad(ids, (\u001b[38;5;241m0\u001b[39m, max_seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(ids)), value\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_id) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m input_ids_list]\n\u001b[1;32m     34\u001b[0m padded_input_ids \u001b[38;5;241m=\u001b[39m pad_sequence(padded_or_truncated_input_ids, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_id)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: padded_input_ids}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(1):  # For simplicity, assuming 1 epoch\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        \n",
    "        # Call the model's forward method with the necessary parameters\n",
    "        outputs = model(input_ids, start_pos=0)\n",
    "\n",
    "        # Assuming that your model returns a dictionary with 'loss'\n",
    "        loss = outputs['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Batch {i}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'llm_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
