{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from fairscale.nn.model_parallel import initialize_model_parallel\n",
    "from model import Transformer, ModelArgs\n",
    "from tokenizer import Tokenizer  # Assuming your tokenizer script is named tokenizer.py\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PROC = 24\n",
    "BATCH_SIZE = 8  # Reduced batch size\n",
    "MAX_SEQ_LEN = 1024  # Reduced sequence length\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"wikipedia\", language=\"en\", date=\"20240401\", split='train[:5%]', trust_remote_code=True, num_proc=NUM_PROC)\n",
    "tokenizer_path = 'cl100k_base.tiktoken'\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    input_ids = [tokenizer.encode(text, bos=True, eos=True) for text in examples['text']]\n",
    "    return {'input_ids': input_ids}\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=NUM_PROC)\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids'])\n",
    "\n",
    "def collate_batch(batch):\n",
    "    input_ids_list = [item['input_ids'].clone().detach().to(torch.long) for item in batch]\n",
    "    padded_input_ids = [\n",
    "        ids[:MAX_SEQ_LEN] if len(ids) > MAX_SEQ_LEN else F.pad(ids, (0, MAX_SEQ_LEN - len(ids)), value=tokenizer.pad_id)\n",
    "        for ids in input_ids_list\n",
    "    ]\n",
    "    return {'input_ids': pad_sequence(padded_input_ids, batch_first=True, padding_value=tokenizer.pad_id)}\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Model Layers and Parameters:\n",
      "tok_embeddings.weight, Shape: torch.Size([100512, 512]), Parameters: 51462144\n",
      "layers.0.attention.wq.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.0.attention.wk.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.0.attention.wv.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.0.attention.wo.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.0.feed_forward.w1.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.0.feed_forward.w2.weight, Shape: torch.Size([512, 5632]), Parameters: 2883584\n",
      "layers.0.feed_forward.w3.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.0.attention_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.0.ffn_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.1.attention.wq.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.1.attention.wk.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.1.attention.wv.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.1.attention.wo.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.1.feed_forward.w1.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.1.feed_forward.w2.weight, Shape: torch.Size([512, 5632]), Parameters: 2883584\n",
      "layers.1.feed_forward.w3.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.1.attention_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.1.ffn_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.2.attention.wq.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.2.attention.wk.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.2.attention.wv.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.2.attention.wo.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.2.feed_forward.w1.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.2.feed_forward.w2.weight, Shape: torch.Size([512, 5632]), Parameters: 2883584\n",
      "layers.2.feed_forward.w3.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.2.attention_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.2.ffn_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.3.attention.wq.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.3.attention.wk.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.3.attention.wv.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.3.attention.wo.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.3.feed_forward.w1.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.3.feed_forward.w2.weight, Shape: torch.Size([512, 5632]), Parameters: 2883584\n",
      "layers.3.feed_forward.w3.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.3.attention_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.3.ffn_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.4.attention.wq.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.4.attention.wk.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.4.attention.wv.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.4.attention.wo.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.4.feed_forward.w1.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.4.feed_forward.w2.weight, Shape: torch.Size([512, 5632]), Parameters: 2883584\n",
      "layers.4.feed_forward.w3.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.4.attention_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.4.ffn_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.5.attention.wq.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.5.attention.wk.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.5.attention.wv.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.5.attention.wo.weight, Shape: torch.Size([512, 512]), Parameters: 262144\n",
      "layers.5.feed_forward.w1.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.5.feed_forward.w2.weight, Shape: torch.Size([512, 5632]), Parameters: 2883584\n",
      "layers.5.feed_forward.w3.weight, Shape: torch.Size([5632, 512]), Parameters: 2883584\n",
      "layers.5.attention_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "layers.5.ffn_norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "norm.weight, Shape: torch.Size([512]), Parameters: 512\n",
      "output.weight, Shape: torch.Size([100512, 512]), Parameters: 51462144\n",
      "Total Parameters: 161126912\n"
     ]
    }
   ],
   "source": [
    "# Set up distributed environment\n",
    "def setup_distributed(world_size=1, rank=0):\n",
    "    if not dist.is_initialized():\n",
    "        dist.init_process_group(\n",
    "            backend='nccl',  # Use 'nccl' for GPUs, 'gloo' for CPU or multi-GPU setups\n",
    "            init_method='tcp://localhost:23456',  # Address for initializing communication\n",
    "            world_size=world_size,  # Total number of processes\n",
    "            rank=rank  # Rank of the current process\n",
    "        )\n",
    "        # Initialize model parallelism\n",
    "        model_parallel_size = 1  # Adjust this as per your setup\n",
    "        initialize_model_parallel(model_parallel_size)\n",
    "\n",
    "setup_distributed()\n",
    "\n",
    "model_args = ModelArgs(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    dim=512,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    ffn_dim_multiplier=4\n",
    ")\n",
    "model = Transformer(model_args)\n",
    "\n",
    "def print_model_details(model):\n",
    "    total_params = 0\n",
    "    print(\"Model Layers and Parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "        print(f\"{name}, Shape: {param.size()}, Parameters: {param_count}\")\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "\n",
    "\n",
    "print_model_details(model)  # Function defined in previous messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.13 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 17.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, start_pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Outputs are logits\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Shift labels to match the prediction shift: predict next token\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m shift_logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     24\u001b[0m shift_labels \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Flatten the logits and labels to fit into cross_entropy\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.13 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 17.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "# Assuming you've already initialized your model, dataloader, and other components\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(1):  # Assuming you want to train for a certain number of epochs\n",
    "    accumulation_steps = 4\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = input_ids.clone()  # Assuming the labels are the input_ids for an auto-regressive model\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, start_pos=0)  # Outputs are logits\n",
    "\n",
    "        # Shift labels to match the prediction shift: predict next token\n",
    "        shift_logits = outputs[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        # Flatten the logits and labels to fit into cross_entropy\n",
    "        loss = cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Batch {i}, Loss: {loss.item()}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'llm_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
